{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the latest cached version of the module from /Users/aldofebri/.cache/huggingface/modules/datasets_modules/datasets/katanaml--cord/fa0960248a7d19cf19675785d5d3dd9eab83b4aea9274b97943d534be56d8a91 (last modified on Fri Jan 24 13:25:45 2025) since it couldn't be found locally at katanaml/cord, or remotely on the Hugging Face Hub.\n",
      "Repo card metadata block was not found. Setting CardData to empty.\n"
     ]
    }
   ],
   "source": [
    "datasets = load_dataset(\"katanaml/cord\", split='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '0',\n",
       " 'words': ['TAX',\n",
       "  '5.455',\n",
       "  'TOTAL',\n",
       "  '60.000',\n",
       "  '(Qty',\n",
       "  '2.00',\n",
       "  'EDC',\n",
       "  'CIMB',\n",
       "  'NIAGA',\n",
       "  'No:',\n",
       "  'xx7730',\n",
       "  '60.000',\n",
       "  '901016',\n",
       "  '-TICKET',\n",
       "  'CP',\n",
       "  '2',\n",
       "  '60.000',\n",
       "  '60.000',\n",
       "  'Subtotal',\n",
       "  '60.000',\n",
       "  'TOTAL',\n",
       "  'DISC',\n",
       "  '$',\n",
       "  '-60.000'],\n",
       " 'bboxes': [[490, 743, 555, 774],\n",
       "  [722, 746, 819, 777],\n",
       "  [101, 827, 282, 858],\n",
       "  [648, 820, 851, 856],\n",
       "  [314, 820, 444, 856],\n",
       "  [479, 822, 615, 851],\n",
       "  [138, 904, 203, 938],\n",
       "  [208, 904, 287, 935],\n",
       "  [291, 901, 388, 935],\n",
       "  [393, 901, 453, 932],\n",
       "  [458, 898, 578, 929],\n",
       "  [731, 901, 847, 932],\n",
       "  [97, 604, 212, 635],\n",
       "  [337, 595, 462, 626],\n",
       "  [467, 595, 513, 626],\n",
       "  [208, 641, 231, 669],\n",
       "  [402, 632, 518, 663],\n",
       "  [703, 632, 814, 663],\n",
       "  [407, 783, 550, 814],\n",
       "  [708, 783, 824, 817],\n",
       "  [337, 709, 435, 740],\n",
       "  [435, 709, 520, 736],\n",
       "  [523, 709, 550, 734],\n",
       "  [687, 709, 819, 737]],\n",
       " 'ner_tags': [14,\n",
       "  14,\n",
       "  22,\n",
       "  22,\n",
       "  19,\n",
       "  19,\n",
       "  17,\n",
       "  17,\n",
       "  17,\n",
       "  17,\n",
       "  17,\n",
       "  17,\n",
       "  4,\n",
       "  3,\n",
       "  3,\n",
       "  1,\n",
       "  5,\n",
       "  0,\n",
       "  13,\n",
       "  13,\n",
       "  10,\n",
       "  10,\n",
       "  10,\n",
       "  10],\n",
       " 'image_path': '/storage/hf-datasets-cache/all/datasets/90316905512161-config-parquet-and-info-katanaml-cord-783b2a2b/downloads/extracted/7d68bbd019d12b4f42efcc573551e6f5a0dc1026e8fb92be9b0f12b791c50a2f/CORD/test/image/receipt_00000.png'}"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import LayoutLMv2Processor, LayoutLMv2ForTokenClassification\n",
    "\n",
    "processor = LayoutLMv2Processor.from_pretrained(\"microsoft/layoutlmv2-base-uncased\", revision=\"no_ocr\")\n",
    "model = LayoutLMv2ForTokenClassification.from_pretrained(\"katanaml/layoutlmv2-finetuned-cord\")\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "receipt_00000.png\n",
      "receipt_00001.png\n",
      "receipt_00002.png\n",
      "receipt_00003.png\n",
      "receipt_00004.png\n",
      "receipt_00005.png\n",
      "receipt_00006.png\n",
      "receipt_00007.png\n",
      "receipt_00008.png\n",
      "receipt_00009.png\n",
      "receipt_00010.png\n",
      "receipt_00011.png\n",
      "receipt_00012.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "receipt_00013.png\n",
      "receipt_00014.png\n",
      "receipt_00015.png\n",
      "receipt_00016.png\n",
      "receipt_00017.png\n",
      "receipt_00018.png\n",
      "receipt_00019.png\n",
      "receipt_00020.png\n",
      "receipt_00021.png\n",
      "receipt_00022.png\n",
      "receipt_00023.png\n",
      "receipt_00024.png\n",
      "receipt_00025.png\n",
      "receipt_00026.png\n",
      "receipt_00027.png\n",
      "receipt_00028.png\n",
      "receipt_00029.png\n",
      "receipt_00030.png\n",
      "receipt_00031.png\n",
      "receipt_00032.png\n",
      "receipt_00033.png\n",
      "receipt_00034.png\n",
      "receipt_00035.png\n",
      "receipt_00036.png\n",
      "receipt_00037.png\n",
      "receipt_00038.png\n",
      "receipt_00039.png\n",
      "receipt_00040.png\n",
      "receipt_00041.png\n",
      "receipt_00042.png\n",
      "receipt_00043.png\n",
      "receipt_00044.png\n",
      "receipt_00045.png\n",
      "receipt_00046.png\n",
      "receipt_00047.png\n",
      "receipt_00048.png\n",
      "receipt_00049.png\n",
      "receipt_00050.png\n",
      "receipt_00051.png\n",
      "receipt_00052.png\n",
      "receipt_00053.png\n",
      "receipt_00054.png\n",
      "receipt_00055.png\n",
      "receipt_00056.png\n",
      "receipt_00057.png\n",
      "receipt_00058.png\n",
      "receipt_00059.png\n",
      "receipt_00060.png\n",
      "receipt_00061.png\n",
      "receipt_00062.png\n",
      "receipt_00063.png\n",
      "receipt_00064.png\n",
      "receipt_00065.png\n",
      "receipt_00066.png\n",
      "receipt_00067.png\n",
      "receipt_00068.png\n",
      "receipt_00069.png\n",
      "receipt_00070.png\n",
      "receipt_00071.png\n",
      "receipt_00072.png\n",
      "receipt_00073.png\n",
      "receipt_00074.png\n",
      "receipt_00075.png\n",
      "receipt_00076.png\n",
      "receipt_00077.png\n",
      "receipt_00078.png\n",
      "receipt_00079.png\n",
      "receipt_00080.png\n",
      "receipt_00081.png\n",
      "receipt_00082.png\n",
      "receipt_00083.png\n",
      "receipt_00084.png\n",
      "receipt_00085.png\n",
      "receipt_00086.png\n",
      "receipt_00087.png\n",
      "receipt_00088.png\n",
      "receipt_00089.png\n",
      "receipt_00090.png\n",
      "receipt_00091.png\n",
      "receipt_00092.png\n",
      "receipt_00093.png\n",
      "receipt_00094.png\n",
      "receipt_00095.png\n",
      "receipt_00096.png\n",
      "receipt_00097.png\n",
      "receipt_00098.png\n",
      "receipt_00099.png\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "expected_list = []\n",
    "predicted_list = []\n",
    "for data in datasets:\n",
    "    image_file = data['image_path'].split(\"image/\")[-1]\n",
    "    # image_file = 'receipt_00012.png'\n",
    "    print(image_file)\n",
    "    image = Image.open(\"image/\" + image_file)\n",
    "    words = data['words']\n",
    "    bbox = data['bboxes']\n",
    "    encoded_inputs = processor(image, words, boxes=bbox, return_offsets_mapping=True, return_tensors=\"pt\")\n",
    "    offset_mapping = encoded_inputs.pop('offset_mapping')\n",
    "    for k,v in encoded_inputs.items():\n",
    "        encoded_inputs[k] = v.to(device)\n",
    "    # load the fine-tuned model from the hub\n",
    "    id2label = model.config.id2label\n",
    "    model.to(device)\n",
    "\n",
    "    # forward pass\n",
    "    outputs = model(**encoded_inputs)\n",
    "    predictions = outputs.logits.argmax(-1).squeeze().tolist()\n",
    "    is_subword = np.array(offset_mapping.squeeze().tolist())[:,0] != 0\n",
    "    true_predictions = [pred for idx, pred in enumerate(predictions) if not is_subword[idx]][1:-1]\n",
    "    expected = data['ner_tags']\n",
    "    if len(expected) != len(true_predictions):\n",
    "        continue\n",
    "        print(f\"Length mismatch: Expected {len(expected)} tokens, but got {len(true_predictions)} tokens, {image_file}\")\n",
    "    else:\n",
    "        expected_list.extend(expected)\n",
    "        predicted_list.extend(true_predictions)\n",
    "    # Break after first iteration for debugging purposes\n",
    "    # break;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2299 2299\n"
     ]
    }
   ],
   "source": [
    "print(len(expected_list), len(predicted_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "204 5 1 2089\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "conf_matrix = confusion_matrix(expected_list, predicted_list)\n",
    "\n",
    "TP, FP, FN, TN = [], [], [], []\n",
    "\n",
    "for i in range(len(conf_matrix)):\n",
    "    tp = conf_matrix[i, i]\n",
    "    TP.append(tp)\n",
    "    \n",
    "    fp = conf_matrix[:, i].sum() - tp\n",
    "    FP.append(fp)\n",
    "    \n",
    "    fn = conf_matrix[i, :].sum() - tp\n",
    "    FN.append(fn)\n",
    "    \n",
    "    tn = conf_matrix.sum() - (conf_matrix[i, :].sum() + conf_matrix[:, i].sum() - tp)\n",
    "    TN.append(tn)\n",
    "print(tp, fp, fn, tn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9973901696389734 0.9760765550239234 0.9951219512195122 0.9855072463768115\n"
     ]
    }
   ],
   "source": [
    "def calculate_accuracy(tp, fp, fn, tn):\n",
    "    return (tp + tn) / (tp + fp + fn + tn)\n",
    "\n",
    "def calculate_precision(tp, fp):\n",
    "    return tp / (tp + fp) if (tp + fp) != 0 else 0\n",
    "\n",
    "def calculate_recall(tp, fn):\n",
    "    return tp / (tp + fn) if (tp + fn) != 0 else 0\n",
    "\n",
    "def calculate_f1(precision, recall):\n",
    "    return 2 * (precision * recall) / (precision + recall) if (precision + recall) != 0 else 0\n",
    "\n",
    "acc = calculate_accuracy(tp, fp, fn, tn)\n",
    "prec = calculate_precision(tp, fp)\n",
    "recall = calculate_recall(tp, fn)\n",
    "f1 = calculate_f1(prec, recall)\n",
    "print(acc, prec, recall, f1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.10.4",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
